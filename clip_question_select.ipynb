{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f82914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use CLIP score to further select questions that possess stronger probing ability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a592c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms.functional import crop\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "from infer_utili.data_utili import get_data, read_json, save_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f4ffcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_id = 3\n",
    "DEVICE = torch.device(f'cuda:{gpu_id}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cff507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CLIP model \n",
    "clip_model = CLIPModel.from_pretrained(\"openai_clip-vit-large-patch14-336\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai_clip-vit-large-patch14-336\")\n",
    "clip_model.eval().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb362024",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_similarity(model, processor, img_patch, text):\n",
    "\n",
    "    inputs = processor(text=text, images=img_patch, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "\n",
    "    # Get embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        image_embeds = outputs.image_embeds  # (1, 768)\n",
    "        text_embeds = outputs.text_embeds    # (1, 768)\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    image_embeds = image_embeds / image_embeds.norm(dim=-1, keepdim=True)\n",
    "    text_embeds = text_embeds / text_embeds.norm(dim=-1, keepdim=True)\n",
    "    similarity = (image_embeds @ text_embeds.T).item()\n",
    "\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f6d8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model = ['llava_v1_5_7b', 'llama_adapter_v2', 'MiniGPT4'][0]\n",
    "filter_apply = ['noFilter', 'withFilter'][0]\n",
    "used_dataset = ['img_Flickr', 'img_dalle'][0]\n",
    "print(filter_apply, used_dataset, target_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba6ea36",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset, dataset_length = get_data(used_dataset)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f595921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check confuser \n",
    "confuser_add = f'ObjColor_exp/confuser_res/{filter_apply}/{used_dataset}_by_gpt-4o-mini_mode/res.json'\n",
    "confuser_res = read_json(confuser_add)\n",
    "print(len(confuser_res))\n",
    "print(confuser_res[0].keys())\n",
    "print(len(confuser_res[0]['sam_result']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9171a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "INFERENCE_FUNCS_sample = {\n",
    "    'llava_v1_5_7b': llava_oneSample_inference,\n",
    "    'MiniGPT4': minigpt4_oneSample_inference,\n",
    "    'llama_adapter_v2': llama_adapter_oneSample_inference,\n",
    "}\n",
    "\n",
    "prompt = 'Describe this image in detail.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa950f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对每张图片 每个检测出来的物体 (对应一个图片patch) 计算 clip 分数, 加入 sam_result 保存信息\n",
    "for sample_id in tqdm(range(dataset_length)):   \n",
    "\n",
    "    image = dataset[sample_id]['image']\n",
    "    # get image description from target model\n",
    "    desc_text = inference_func(model_dict, prompt, image, args)[0]\n",
    "\n",
    "    obj_info = confuser_res[sample_id]['sam_result']\n",
    "    if obj_info is not None:\n",
    "        for obj_sam_res in obj_info:\n",
    "            box_info = obj_sam_res['bbox']\n",
    "            # obtain image patch based on box \n",
    "            img_patch = image.crop(box_info)\n",
    "            clip_score = clip_similarity(clip_model, clip_processor, img_patch, desc_text)\n",
    "            obj_sam_res['clip_score'] = clip_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed34e8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52d266b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d65042b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3fcff6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6149579",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91110b4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa0213a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3292c42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcdae83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

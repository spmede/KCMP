{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91345d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rank clip score to select part of the questions for characteristic score calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9509ed6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "import numpy as np\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "from infer_utili.data_utili import get_data, read_json, save_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49af340d",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model = ['llava_v1_5_7b', 'llama_adapter_v2', 'MiniGPT4'][0]\n",
    "used_dataset = ['img_Flickr', 'img_dalle'][0]\n",
    "\n",
    "filter_apply = ['noFilter', 'withFilter'][0]\n",
    "ask_type = ['ordered_choice', 'random_choice'][0]\n",
    "print(f'{used_dataset}\\n{target_model}\\n\\n{filter_apply}\\n{ask_type}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402e4c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get confuser res generated from `get_clipScore.py`\n",
    "confuser_res_add = f'ObjColor_exp/confuser_res/{filter_apply}/{used_dataset}_by_gpt-4o-mini/{target_model}/res_w_clip.json'\n",
    "confuser_res = read_json(confuser_res_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2e2618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify the res add based on your actual directory  \n",
    "traverse_res_add = f'ObjColor_exp/traverse_res/{filter_apply}/{used_dataset}/{target_model}/{ask_type}/temp_0.3'\n",
    "traverse_res = read_json(traverse_res_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a30a805",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_clip_score_dict(confuser_entries):\n",
    "    \"\"\"\n",
    "    Build a dict for fast lookup: (img_id, object_index) -> clip_score\n",
    "    \"\"\"\n",
    "    score_dict = {}\n",
    "    for entry in confuser_entries:\n",
    "        img_id = entry[\"original_img_id\"]\n",
    "        for i, sam in enumerate(entry.get(\"sam_result\", [])):\n",
    "            score_dict[(img_id, i)] = sam.get(\"clip_score\", 0.0)\n",
    "    return score_dict\n",
    "\n",
    "def compute_auc(grouped_data, question_type, ignore_invalid, topk=None, clip_score_lookup=None):\n",
    "    scores = []\n",
    "    labels = []\n",
    "    label0_ids = []\n",
    "    label1_ids = []\n",
    "\n",
    "    for entry in grouped_data:\n",
    "        label = entry[\"ground_truth_label\"]\n",
    "        img_id = entry[\"img_id\"]\n",
    "\n",
    "        # select only questions of target type and (optionally) valid\n",
    "        target_questions = [\n",
    "            q for q in entry[\"questions\"]\n",
    "            if q[\"question_type\"] == question_type and (not ignore_invalid or q[\"invalid_info\"] == 0)\n",
    "        ]\n",
    "\n",
    "        if not target_questions:\n",
    "            continue\n",
    "\n",
    "        # inject clip_score if possible\n",
    "        if clip_score_lookup:\n",
    "            for q in target_questions:\n",
    "                key = (img_id, q[\"object_index\"])\n",
    "                q[\"clip_score\"] = clip_score_lookup.get(key, 0.0)\n",
    "\n",
    "            target_questions.sort(key=lambda q: q[\"clip_score\"], reverse=True)\n",
    "\n",
    "        if topk is not None:\n",
    "            target_questions = target_questions[:topk]\n",
    "\n",
    "        image_score = sum(q[\"acc\"] for q in target_questions) / len(target_questions)\n",
    "        scores.append(image_score)\n",
    "        labels.append(label)\n",
    "\n",
    "        if label == 0:\n",
    "            label0_ids.append(img_id)\n",
    "        else:\n",
    "            label1_ids.append(img_id)\n",
    "\n",
    "    if len(set(labels)) < 2:\n",
    "        print(\"[WARN] Only one label class found, AUC cannot be computed.\")\n",
    "        return None, len(label0_ids), len(label1_ids)\n",
    "\n",
    "    auc = roc_auc_score(labels, scores)\n",
    "    return auc, len(label0_ids), len(label1_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c317cbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_auc_analysis(confuser_res, traverse_res, topk_for_auc):\n",
    "    # === Run CLIP-based AUC evaluation ===\n",
    "    print(f\"[Num={topk_for_auc}] Running CLIP-sorted AUC evaluation...\")\n",
    "    confuser_json_for_auc = True\n",
    "\n",
    "    if confuser_json_for_auc:\n",
    "        clip_confuser = confuser_res\n",
    "        clip_score_lookup = {\n",
    "            (entry[\"original_img_id\"], i): sam.get(\"clip_score\", 0.0)\n",
    "            for entry in clip_confuser\n",
    "            for i, sam in enumerate(entry[\"sam_result\"] or [])\n",
    "        }\n",
    "    else:\n",
    "        clip_score_lookup = None\n",
    "\n",
    "    for qtype in [\"ask_masked_obj\", \"ask_obj_color\", \"both\"]:\n",
    "        for ignore_flag in [False, True]:\n",
    "            print(f\"\\n>>> AUC Eval | qtype={qtype} | ignore_invalid={ignore_flag} | topk={topk_for_auc}\")\n",
    "\n",
    "            if qtype == \"both\":\n",
    "                def merged_clip_auc(data, ignore_invalid, topk, clip_score_lookup):\n",
    "                    scores, labels, label0_ids, label1_ids = [], [], [], []\n",
    "                    for entry in data:\n",
    "                        label = entry[\"ground_truth_label\"]\n",
    "                        img_id = entry[\"img_id\"]\n",
    "                        valid_qs = [\n",
    "                            q for q in entry[\"questions\"]\n",
    "                            if not ignore_invalid or q[\"invalid_info\"] == 0\n",
    "                        ]\n",
    "                        if not valid_qs:\n",
    "                            continue\n",
    "\n",
    "                        # add clip_score\n",
    "                        if clip_score_lookup:\n",
    "                            for q in valid_qs:\n",
    "                                q[\"clip_score\"] = clip_score_lookup.get((img_id, q[\"object_index\"]), 0.0)\n",
    "                            valid_qs.sort(key=lambda q: q[\"clip_score\"], reverse=True)\n",
    "\n",
    "                        if topk is not None:\n",
    "                            valid_qs = valid_qs[:topk]\n",
    "\n",
    "                        image_score = sum(q[\"acc\"] for q in valid_qs) / len(valid_qs)\n",
    "                        scores.append(image_score)\n",
    "                        labels.append(label)\n",
    "                        (label0_ids if label == 0 else label1_ids).append(img_id)\n",
    "\n",
    "                    if len(set(labels)) < 2:\n",
    "                        print(\"[WARN] Only one label class found. Skipping.\")\n",
    "                        return None, len(label0_ids), len(label1_ids)\n",
    "                    auc = roc_auc_score(labels, scores)\n",
    "                    return auc, len(label0_ids), len(label1_ids)\n",
    "\n",
    "                auc, n0, n1 = merged_clip_auc(traverse_res, ignore_flag, topk_for_auc, clip_score_lookup)\n",
    "            else:\n",
    "                auc, n0, n1 = compute_auc(\n",
    "                    grouped_data=traverse_res,\n",
    "                    question_type=qtype,\n",
    "                    ignore_invalid=ignore_flag,\n",
    "                    topk=topk_for_auc,\n",
    "                    clip_score_lookup=clip_score_lookup\n",
    "                )\n",
    "\n",
    "            if auc is not None:\n",
    "                print(f\"[AUC] {auc:.4f} | label0: {n0}, label1: {n1}\")\n",
    "            else:\n",
    "                print(\"[AUC] N/A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e80b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the number of used questions per image\n",
    "topk_for_auc = 5\n",
    "num_auc_analysis(confuser_res, traverse_res, topk_for_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752775f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
